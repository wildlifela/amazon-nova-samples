{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7f661d3",
   "metadata": {},
   "source": [
    "# Amazon Bedrock Batch inference with Amazon Nova FMs to analyze videos - Work with videos on a large scale\n",
    "This notebook walks through the end-to-end process of running batch inference on a collection of videos stored in Amazon S3 using Amazon Nova Pro or Amazon Nova Premier via Amazon Bedrock.\n",
    "\n",
    "With [Batch Inference](https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference.html), you can provide a set of prompts as a single input file and receive responses as a single output file, allowing you to get simultaneous large-scale predictions. The responses are processed and stored in your Amazon S3 bucket so you can access them at a later time. Amazon Bedrock offers support for Amazon Nova Foundation Models (FMs) for batch inference at a 50% lower price compared to on-demand inference pricing. Please refer to model list [here](https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference-supported.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14a007b",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "**Amazon Bedrock** provides a unified API for invoking foundation models like Amazon Nova Lite, Amazon Nova Pro or Amazon Nova Premier as described [here](https://docs.aws.amazon.com/nova/latest/userguide/what-is-nova.html). When you want to analyze, summarize, caption, or classify a **large set of videos** you can use **batch inference**. This notebook shows you how to:\n",
    "\n",
    "1. Discover all your MP4 videos in Amazon S3\n",
    "2. Build a JSONL payload referencing them\n",
    "3. Upload payload to Amazon S3\n",
    "4. Kick off a Amazon Nova batch inference job\n",
    "5. Poll for completion\n",
    "6. Download the results locally and explore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad9654f",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Prerequisites](#prerequisites)\n",
    "2. [Install Dependencies](#install-dependencies)\n",
    "3. [Configuration & Imports](#configuration--imports)\n",
    "4. [Helper Functions](#helper-functions)\n",
    "5. [List Videos in S3](#list-videos-in-s3)\n",
    "6. [Build JSONL Payload](#build-jsonl-payload)\n",
    "7. [Upload JSONL to S3](#upload-jsonl-to-s3)\n",
    "8. [Invoke Batch Job](#invoke-batch-job)\n",
    "9. [Poll Job Status](#poll-job-status)\n",
    "10. [Download Results](#download-results)\n",
    "11. [Conclusion](#conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad17540f",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "Before you begin, ensure that you have the following prerequisites in place:\n",
    "\n",
    "1. You must have permissions to invoke `CreateModelInvocationJob` and `GetModelInvocationJob` API. Refer to the documentation to learn about [required permissions for batch inference job](https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference-prereq.html#batch-inference-permissions).\n",
    "2. Provide a S3 bucket with empty prefixes for `video/batch/input/` and `video/batch/output/`.\n",
    "3. Bedrock Batch Inference requires a service role so that it can access and write to S3 on your behalf. You can create the service role manually [see here](https://docs.aws.amazon.com/bedrock/latest/userguide/batch-iam-sr.html) or use the AWS Console workflow which can create one for you [here](https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/batch-inference/create). We also provide a quick way to create a service role in the code below. The role requires permissions to read and write data on Amazon S3 bucket (`GetObject`, `ListBucket`, `PutObject`).\n",
    "4. This notebook was built using videos with `.mp4` format.\n",
    "5. Ensure that you are in a AWS region that is supported for Batch Inference. Refer [here](https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference-supported.html) for documentation.\n",
    "6. The default maximum size of a single file (in GB) submitted for batch inference for Nova models is 1 GB. However, you can request an increase [here](https://us-east-1.console.aws.amazon.com/servicequotas/home/services/bedrock/quotas/L-68FC8D47) as needed.\n",
    "7. Amazon Bedrock Batch Inference requires a minimum of 100 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f700b7c-e4ff-4589-8674-f08709ca0be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet \"boto3>=1.35.1\" huggingface_hub sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc01a2e-7a33-48ae-8cc0-3f596a75e8b0",
   "metadata": {},
   "source": [
    "Next we import the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4740c261-ad12-4c97-ac56-57719a716211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import re\n",
    "import html\n",
    "import os\n",
    "import time\n",
    "from IPython.display import display, Markdown, HTML\n",
    "from urllib.parse import urlparse\n",
    "from botocore.config import Config as BConfig\n",
    "import boto3\n",
    "from huggingface_hub import snapshot_download\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f882a33-8a93-424c-a356-33dbeaf0d569",
   "metadata": {},
   "source": [
    "Please update the variables below to be able to run inside your AWS account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a935a64a-a290-4d1e-bab2-b66cd4f8292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'YOUR_AMAZON_S3_BUCKET_NAME' # REPLACE this bucket name with the name of your Amazon S3 bucket\n",
    "ROLE_ARN = 'ROLE_ARN' # REPLACE with the arn of the IAM service role that you created in the prerequisites\n",
    "MODEL_ID = 'arn:aws:bedrock:us-east-1::foundation-model/amazon.nova-pro-v1:0' # the arn of the model. Change this if you want to use a different model or a different region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb41a2b6-d8b4-4644-a886-2343b8ff7ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"./local_data\" # if you want you can point this to a local directory that contains videos to process\n",
    "VIDEOS_SOURCE_PREFIX = 'video/batch/source/academic_source'\n",
    "INPUT_PREFIX = 'video/batch/input'\n",
    "OUTPUT_PREFIX = 'video/batch/output'\n",
    "MAX_TOKENS = 200\n",
    "OUTPUT_FOLDER = os.path.join(DATASET_DIR,'outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af47ce3-126d-4eb6-bcc5-3b5c44629593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the Amazon SageMaker Python SDK to upload to Amazon S3 buckets\n",
    "session = boto3.session.Session(region_name=\"us-east-1\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab0ffa-7ec6-4b03-aa68-3931da65ef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize AWS clients\n",
    "boto_cfg = BConfig(retries={'max_attempts':10,'mode':'standard'})\n",
    "s3 = session.client('s3', config=boto_cfg)\n",
    "bedrock = session.client('bedrock', config=boto_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163cbe7e-baac-495c-b128-627e9fdfc838",
   "metadata": {},
   "source": [
    "### Download Sample Dataset\n",
    "\n",
    "Download a dataset from the HuggingFace Hub as an example. \n",
    "\n",
    "If you want to use your own videos you can skip this step and instead create a directory `./local_data`, place your videos in that directory and upload them directly to the Amazon S3 bucket in the next step. \n",
    "\n",
    "We will use a subset of videos from the [LLaVa-Video-178k](https://huggingface.co/datasets/lmms-lab/LLaVA-Video-178K) as sample video input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410cebca-66c2-4991-af7b-83b649aed599",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #006CE0; \n",
    "    padding: 10px; \n",
    "    border-radius: 5px; \n",
    "    max-width: 100%;\n",
    "    background: #f0fbff;\">\n",
    "    <b>Info:</b> Dataset <a href=\"https://huggingface.co/datasets/lmms-lab/LLaVA-Video-178K\">lmms-lab/LLaVA-Video-178K</a> is released to public under <a href=\"https://www.apache.org/licenses/LICENSE-2.0\">Apache 2.0 License</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b300cf-3c29-40f4-bd34-174475f897b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = snapshot_download(\n",
    "    repo_id=\"malterei/LLaVA-Video-small-swift\",\n",
    "    repo_type=\"dataset\",\n",
    "    allow_patterns=[\n",
    "        \"videos/academic_source/Charades/*\",\n",
    "        \"videos/academic_source/youcook2/*\",\n",
    "        \"videos/academic_source/activitynet/*\",\n",
    "    ],\n",
    "    local_dir=DATASET_DIR\n",
    ")\n",
    "print(f\"Downloaded dataset to local filepath: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7296de-7678-49b3-9017-dea96f42dfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's move the videos into the root of ./local_data\n",
    "!mv {DATASET_DIR}/videos/academic_source/* {DATASET_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725e53fe-e554-44c7-9552-7974ccaf5312",
   "metadata": {},
   "source": [
    "### Upload videos to your Amazon S3 bucket\n",
    "Upload the whole folder **./local_data** to an Amazon S3 bucket.\n",
    "In case you want to use a different folder structure in Amazon S3, you have to change the variable **VIDEOS_SOURCE_PREFIX**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e7ea51-30e6-4b18-a30e-faf887fecdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_s3_uri = sagemaker_session.upload_data(\n",
    "    DATASET_DIR, \n",
    "    bucket=BUCKET_NAME, \n",
    "    key_prefix=VIDEOS_SOURCE_PREFIX,\n",
    ")\n",
    "print(f\"Uploaded data from local: {file_path} to s3: {dataset_s3_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2c55c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper Functions\n",
    "def list_mp4s_in_s3(bucket, prefix):\n",
    "    \"\"\"\n",
    "    Recursively list all .mp4 object keys under a given S3 prefix.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Listing MP4s under s3://{bucket}/{prefix}/\")\n",
    "    token = None\n",
    "    keys = []\n",
    "    while True:\n",
    "        params = {'Bucket': bucket, 'Prefix': prefix}\n",
    "        if token:\n",
    "            params['ContinuationToken'] = token\n",
    "        resp = s3.list_objects_v2(**params)\n",
    "        for obj in resp.get('Contents', []):\n",
    "            k = obj['Key']\n",
    "            if k.lower().endswith('.mp4'):\n",
    "                keys.append(k)\n",
    "                logger.info(f\"  â€¢ {k}\")\n",
    "        if not resp.get('IsTruncated'):\n",
    "            break\n",
    "        token = resp.get('NextContinuationToken')\n",
    "    return keys\n",
    "\n",
    "\n",
    "def pretty_llm_print(video_output, title=None):\n",
    "    \"\"\"Generates formatted output showing the prompt once and then video names and their summaries.\"\"\"\n",
    "    # Header styling\n",
    "    header = \"\"\n",
    "    if title:\n",
    "        header = f\"\"\"<div style='border: 2px solid #000000; \n",
    "            padding: 10px; \n",
    "            border-radius: 5px; \n",
    "            max-width: fit-content; \n",
    "            margin: 0 auto; \n",
    "            text-align: center; \n",
    "            font-weight: bold;'>{title}</div>\\n\"\"\"\n",
    "\n",
    "    body_parts = [header]\n",
    "    \n",
    "    # Display the prompt (user's question) ONCE at the top\n",
    "    if video_output:\n",
    "        user_content = video_output[0]['modelInput']['messages'][0]['content']\n",
    "        prompt_texts = [html.escape(c['text']).replace('\\\\n', '\\n')\n",
    "                        for c in user_content if 'text' in c]\n",
    "        if prompt_texts:\n",
    "            body_parts.append(\"\\n**Prompt:**\\n\\n\")\n",
    "            body_parts.append(\"<div style='margin-bottom: 1em; font-style: italic;'>\")\n",
    "            body_parts.append(\"<br>\".join(prompt_texts))\n",
    "            body_parts.append(\"</div>\")\n",
    "            body_parts.append(\"\\n\\n---\\n\")  # Horizontal rule after prompt\n",
    "\n",
    "\n",
    "    # Process each video entry\n",
    "    for entry in video_output:\n",
    "        # Extract video name from S3 URI\n",
    "        video_uri = entry['modelInput']['messages'][0]['content'][1]['video']['source']['s3Location']['uri']\n",
    "        video_name = video_uri.split(f\"{VIDEOS_SOURCE_PREFIX}/\")[-1]  # Get filename from URI\n",
    "\n",
    "        \n",
    "        # Add video name header\n",
    "        body_parts.append(f\"\\n## ðŸ“¹ {video_name}\\n\")\n",
    "        \n",
    "        # Process assistant summary\n",
    "        assistant_content = entry['modelOutput']['output']['message']['content']\n",
    "        for content in assistant_content:\n",
    "            if 'text' in content:\n",
    "                processed = process_content_string(content['text'])\n",
    "                body_parts.append(processed)\n",
    "        \n",
    "        # Add separator between entries\n",
    "        body_parts.append(\"\\n\\n---\\n\")\n",
    "\n",
    "    # Final styling\n",
    "    styled_markdown = f\"\"\"\n",
    "<div style=\"border: 2px solid #FFC000; \n",
    "    padding: 10px; \n",
    "    border-radius: 5px; \n",
    "    max-width: 100%;\">\n",
    "{''.join(body_parts)}\n",
    "</div>\"\"\"\n",
    "    display(Markdown(styled_markdown))\n",
    "\n",
    "def process_content_string(text):\n",
    "    \"\"\"Format thinking/answer blocks\"\"\"\n",
    "    text = text.replace('\\\\n', '\\n')\n",
    "    \n",
    "    answer_style = \"\"\"<div style=\"background-color: #e8f5e9; \n",
    "        border-left: 4px solid #43a047; \n",
    "        padding: 10px; \n",
    "        margin: 10px 0; \n",
    "        border-radius: 4px;\">\n",
    "        <strong style=\"color: #43a047;\">Summary</strong>\n",
    "        <div style=\"margin-top: 8px; white-space: pre-wrap;\">{}</div>\n",
    "    </div>\"\"\"\n",
    "    \n",
    "    # Convert <answer> tags to summary blocks\n",
    "    text = re.sub(r'<answer>(.*?)</answer>', \n",
    "                 lambda m: answer_style.format(m.group(1)), \n",
    "                 text, \n",
    "                 flags=re.DOTALL)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf3e23",
   "metadata": {},
   "source": [
    "## List videos in Amazon S3\n",
    "Use the helper function to find all MP4s under your source prefix in your Amazon S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6a0a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp4_keys = list_mp4s_in_s3(BUCKET_NAME, VIDEOS_SOURCE_PREFIX)\n",
    "print(f\"Found {len(mp4_keys)} videos.\")\n",
    "mp4_keys[:5]  # show first 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c754ae1",
   "metadata": {},
   "source": [
    "## Build JSONL Payload\n",
    "\n",
    "Amazon Bedrock Batch Inference takes a JSONL file as input. The JSONL contains rows of JSON objects. Each line contains one request to the model:\n",
    "\n",
    "```json\n",
    "{ \"recordId\" : \"alphanumeric string\", \"modelInput\" : {JSON body} }\n",
    "...\n",
    "```\n",
    "\n",
    "In this case with Amazon Nova video understanding the JSON body in the `modelInput` field adheres to the Amazon Nova messages format with video content. It contains a text prompt and points to a video stored on Amazon S3. Here is the schema for the JSON body in `modelInput` that we are using:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"schemaVersion\": \"messages-v1\",\n",
    "    \"messages\":[\n",
    "        {\n",
    "            \"role\":\"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                  \"text\": string\n",
    "                },\n",
    "                {\n",
    "                  \"video\": {\n",
    "                    \"format\": \"mp4\",\n",
    "                    \"source\": {\n",
    "                      \"s3Location\": {\n",
    "                        \"uri\": \"s3://my-bucket/object-key\"\n",
    "                        \"bucketOwner\": \"123456789012\"\n",
    "                       }\n",
    "                    }\n",
    "                  }\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"inferenceConfig\": {\n",
    "        \"maxTokens\": 200\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "You can read more about the request schema in the AWS Documentation:\n",
    "* [Amazon Nova Complete request schema](https://docs.aws.amazon.com/nova/latest/userguide/complete-request-schema.html)\n",
    "* [Format and upload your batch inference data](https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference-data.html)\n",
    "* [Amazon Nova Video understanding examples](https://docs.aws.amazon.com/nova/latest/userguide/modalities-video-examples.html)\n",
    "\n",
    "We define a helper function that converts the list of Amazon S3 keys into a newline-delimited JSONL file for batch input. The text instructions that we send to Amazon Nova alongside the video is the following prompt: \"Please summarize this video in ~200 words.\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827c9c5d-dcd4-4605-8f4a-27a85ad2189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_jsonl_from_s3_keys(keys, bucket):\n",
    "    \"\"\"\n",
    "    Build a list of JSONL-ready records pointing to videos via s3Location.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for idx, k in enumerate(keys):\n",
    "        uri = f\"s3://{bucket}/{k}\"\n",
    "        video_obj = {\n",
    "            'video': {\n",
    "                'format':'mp4', \n",
    "                'source':{\n",
    "                    's3Location':{\n",
    "                        'uri':uri\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        text_obj = {\n",
    "            'text':'Please summarize this video in ~200 words.'\n",
    "        }\n",
    "        rec = {\n",
    "            'recordId': f'video-{idx}',\n",
    "            'modelInput':{\n",
    "                'schemaVersion':'messages-v1',\n",
    "                'messages':[\n",
    "                    {\n",
    "                        'role':'user',\n",
    "                        'content':[\n",
    "                            text_obj, \n",
    "                            video_obj\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                'inferenceConfig':{\n",
    "                    'maxTokens':MAX_TOKENS\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        records.append(rec)\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c0eb9d-64d6-4798-b7dd-25b485371163",
   "metadata": {},
   "source": [
    "Let's use the helper function to create the JSONL file for your Amazon S3 keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c05c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = build_jsonl_from_s3_keys(mp4_keys, BUCKET_NAME)\n",
    "input_key = f\"{INPUT_PREFIX}/video_batch_{int(time.time())}.jsonl\"\n",
    "jsonl_str = '\\n'.join(json.dumps(r) for r in records)\n",
    "print(f\"Generated {len(records)} records â†’ {input_key}\")\n",
    "print(jsonl_str[:500], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6252ae39",
   "metadata": {},
   "source": [
    "## Upload JSONL to Amazon S3\n",
    "The batch inference job will read the JSONL file from Amazon S3. Upload the JSONL to the Amazon S3 bucket below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d003008",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.put_object(Bucket=BUCKET_NAME, Key=input_key, Body=jsonl_str.encode('utf-8'))\n",
    "print(f\"Uploaded JSONL to s3://{BUCKET_NAME}/{input_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da294dc4",
   "metadata": {},
   "source": [
    "## Invoke Amazon Bedrock Batch Inference Job\n",
    "\n",
    "Start the Amazon Bedrock Batch Inference job with Amazon Nova as the foundation model.\n",
    "\n",
    "In the request you specify the input data configuration which is the Amazon S3 bucket that contains the videos and the JSONL file which contains the prompts.\n",
    "\n",
    "The `modelId` specifies which model the batch inference job should use. \n",
    "\n",
    "The request also contains the output location. Amazon Bedrock Batch Inference will write the output from the model to the Amazon S3 bucket that is configured in the output data config. After the batch inference completes you can get the output from that Amazon S3 bucket.\n",
    "\n",
    "As part of the request you also specify the arn of the role that the batch inference job will assume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411a9c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting batch inference job with model: {MODEL_ID}\")\n",
    "resp = bedrock.create_model_invocation_job(\n",
    "    jobName=f\"batch-video-{int(time.time())}\",\n",
    "    modelId=MODEL_ID,\n",
    "    inputDataConfig={ \n",
    "        's3InputDataConfig': {\n",
    "            's3Uri': f\"s3://{BUCKET_NAME}/video/batch/\",\n",
    "            's3InputFormat': 'JSONL'\n",
    "    }},\n",
    "    outputDataConfig={ \n",
    "        's3OutputDataConfig': {\n",
    "            's3Uri': f\"s3://{BUCKET_NAME}/{OUTPUT_PREFIX}/\"\n",
    "        }\n",
    "    },\n",
    "    roleArn=ROLE_ARN\n",
    ")\n",
    "job_arn = resp['jobArn']\n",
    "print(f\"Started job ARN: {job_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463e30b6",
   "metadata": {},
   "source": [
    "## Poll Job Status\n",
    "Wait until the batch job completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43f6525",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    status_resp = bedrock.get_model_invocation_job(jobIdentifier=job_arn)\n",
    "    status = status_resp['status']\n",
    "    print('Status:', status)\n",
    "    if status in ('Completed','Failed'):\n",
    "        break\n",
    "    time.sleep(10)\n",
    "print('Final status:', status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7127afdf",
   "metadata": {},
   "source": [
    "## Download Results\n",
    "Fetch the generated JSONL output files to your local `./outputs/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e88e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "out_uri = status_resp['outputDataConfig']['s3OutputDataConfig']['s3Uri']\n",
    "parsed = urlparse(out_uri)\n",
    "out_bucket, out_prefix = parsed.netloc, parsed.path.lstrip('/')\n",
    "paginator = s3.get_paginator('list_objects_v2')\n",
    "for page in paginator.paginate(Bucket=out_bucket, Prefix=out_prefix):\n",
    "    for obj in page.get('Contents', []):\n",
    "        if not obj['Key'].lower().endswith('.jsonl.out'):\n",
    "            continue\n",
    "        dst = os.path.join(OUTPUT_FOLDER, os.path.basename(obj['Key']))\n",
    "        s3.download_file(out_bucket, obj['Key'], dst)\n",
    "        print('Downloaded â†’', dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ed4f59-e9e8-4da8-82a6-a4ac1d9b6b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract summaries from first output file after batch processing\n",
    "output_files = os.listdir(OUTPUT_FOLDER)\n",
    "objects = []\n",
    "with open(os.path.join(OUTPUT_FOLDER, output_files[0]), 'r') as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        objects.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e71f9eb-e1d6-4e84-aaa9-d53357f2b42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first 5 summaries will be displayed\n",
    "first_five = objects[:5]\n",
    "model_name = MODEL_ID.rpartition('/')[-1]\n",
    "pretty_llm_print(first_five, title=\"Video summaries by batch processing with \" + model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dbdca7-2f3c-4eea-9aa7-454d5eabae09",
   "metadata": {},
   "source": [
    "### Integrating with Existing Workflows\n",
    "\n",
    "After retrieving the processed output data, you can integrate it into your existing workflows or analytics systems for further analysis or downstream processing. For example, you could:\n",
    "\n",
    "- Store the summarized videos in a database for easy access and querying.\n",
    "- Perform sentiment analysis or topic modeling on the summarized transcripts to gain additional insights.\n",
    "- Categorize the summarizes into actionable business buckets.\n",
    "\n",
    "The specific integration steps will depend on your existing workflows and systems, but the processed output data from the batch inference job can be easily incorporated into various data pipelines and analytics processes.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The notebook covers the entire process, from data preparation and formatting to job submission, output retrieval, and integration with existing workflows. You can leverage the JSONL outputs for further analysis or visualization. Feel free to adapt and extend this notebook to suit your specific requirements, and explore other use cases where batch inference can be applied to optimize your interactions with foundation models at scale. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
